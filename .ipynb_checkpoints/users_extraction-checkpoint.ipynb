{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API settup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(\n",
    "    consumer_key = '',\n",
    "    consumer_secret = '',\n",
    "    access_token_key = '',\n",
    "    access_token_secret = '',\n",
    "    sleep_on_rate_limit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original screen names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=b9FHAuO65aw&t=2054s\n",
    "\n",
    "screen_names = [\n",
    "    'BarneriasPierre',    # real of the movie\n",
    "    'holdup_ledoc',       # twitter account of the movie\n",
    "    'silvano_trotta',     # anti-vax, Lune est creuse\n",
    "    'ChroniLyme',         # Christian Perronne, maladie de lyme\n",
    "    'xazalbert',          # Xavier Azalbert, home d'affaire pro-cloroquine, a un média\n",
    "    'CorinneReverbel',    # Elle a relayé de nombreux autres dans la liste\n",
    "    'ViolaineGuerin',     # pro-hydro \n",
    "    'MartineWonner',      # députée, masque ne sert à rien\n",
    "    'TrottaDr',           # pro-raoult, médecine alternative\n",
    "    'RaderSerge',         # anti-vax\n",
    "    'Stuckelberger',      # spécialiste du viellissement, pas de deuxième vague\n",
    "    'EmaKrusi',           # a interviewé nombreux d'entre eux\n",
    "    'laancelot',          # Michel Rozensweig, écrivain pour nexus, magasine de désinformation\n",
    "    'MagazineNexus',      # gros magzine de désinformation, beaucoup on écrit dedans\n",
    "    'PINCON_CHARL0T'      # sociologue, extrême gauche, extermination des pauvres via réchauffement climatique\n",
    "]\n",
    "\n",
    "names = [\n",
    "    'Luc montagnier',            # no twitter account, nobel prize, says COVID is handmade\n",
    "    'jean-bernard fourtillan',   # no twitter account, anti-vax, Dieu lui a révélé \n",
    "    'alexandra henrion-caude',   # Twitter account in english, amourons\n",
    "    'Lauran Toubiana',           # no twitter account,Astrophysicien, mais considéré comme épydémiologiste, pas de 2nd vague\n",
    "    'Edouard Broussalian',       # Twitter account in english, deseases don't exist\n",
    "    'Valérie Bugault',           # no Twitter account, conspirationiste\n",
    "    'Michael Levitt',            # Twitter account in english\n",
    "    'Miguel Barthelery',         # no Twitter account, les virus sont bénéfiques\n",
    "    'Olivier Vuillemin'          # contre 5G\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetches the people that are followed by the original screen names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_friends(api, screen_names):\n",
    "    \"\"\"\n",
    "    returns a dictionary of all friends (people followed by) of all given screen_names,\n",
    "    and count their occurences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        api : python.twitter.Api object\n",
    "            \n",
    "        screen_names : list of String\n",
    "            the screen names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        d : dictionary, keys are the ids of the friends and values are the number of\n",
    "            time they occurred\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "\n",
    "    for screen_name in tqdm(screen_names):\n",
    "        for id_ in api.GetFriendIDs(screen_name = screen_name):\n",
    "            if id_ not in d.keys():\n",
    "                d[id_] = 1\n",
    "            else:\n",
    "                d[id_] = d[id_] + 1\n",
    "            \n",
    "    return d\n",
    "\n",
    "def screen_names_to_ids(api, screen_names):\n",
    "    \"\"\"\n",
    "    Transforms \n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for screen_name in screen_names:\n",
    "        id_ = api.GetUser(screen_name = screen_name).id\n",
    "        res.append(id_)\n",
    "    return res\n",
    "\n",
    "def get_tweets(api, user_ids):\n",
    "    timelines = []\n",
    "    for id_ in tqdm(user_ids):\n",
    "        # just to pass the first while\n",
    "        day = 23\n",
    "        month = 'Nov'\n",
    "        max_id = None\n",
    "        tls = []\n",
    "        # as long as we are between now and when the movie came out, we keep getting older tweets\n",
    "        while(day > 9 and month == 'Nov'):\n",
    "            timeline = api.GetUserTimeline(user_id = id_, count = 200, max_id = max_id)\n",
    "            # We look at the oldest and update the values\n",
    "            last = timeline[-1]\n",
    "            date = last.created_at.split(' ')\n",
    "            day, month = int(date[2]), date[1]\n",
    "            max_id = last.id\n",
    "            # We add the timeline, omitting the last to avoid duplicates\n",
    "            tls.append([x.id for x in timeline[:-1]])\n",
    "\n",
    "        timelines.append(np.concatenate(tls))\n",
    "    \n",
    "    return np.concatenate(timelines)\n",
    "\n",
    "def get_mentions_and_hashtags(api, statuses_ids):\n",
    "    mentions_dict = {}\n",
    "    hashtags_dict = {}\n",
    "\n",
    "    for status in statuses:\n",
    "        mentions = status.user_mentions\n",
    "        hashtags = status.hashtags\n",
    "\n",
    "        for m in mentions:\n",
    "            name = m.screen_name\n",
    "            if name in mentions_dict.keys():\n",
    "                mentions_dict[name] = mentions_dict[name] + 1\n",
    "            else:\n",
    "                mentions_dict[name] = 1\n",
    "\n",
    "        for h in hashtags:\n",
    "            txt = h.text\n",
    "            if txt in hashtags_dict.keys():\n",
    "                hashtags_dict[txt] = hashtags_dict[txt] + 1\n",
    "            else:\n",
    "                hashtags_dict[txt] = 1\n",
    "        \n",
    "    return mentions_dict, hashtags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "ids = get_common_friends(api, screen_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only those that are followed by at least 5 of the original screen names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take only followed by 5 or more\n",
    "res = []\n",
    "for (id_, n) in ids.items():\n",
    "    if n >= 5:\n",
    "        res.append(id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out non-french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take only those in french\n",
    "french_user_ids = []\n",
    "for id_ in res:\n",
    "    usr = api.GetUser(user_id = id_)\n",
    "    if usr.status.lang == 'fr' or usr.lang == 'fr':\n",
    "        french_user_ids.append(id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add previous screen names ids and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add previous screen names ids and remove duplicates\n",
    "friends = screen_names_to_ids(api, screen_names)\n",
    "french_user_ids = np.concatenate((friends, french_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_user_ids = list(set(french_user_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a first pass of posts with the screen names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:32<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "tweet_ids = get_tweets(api, french_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17616"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find hashtags and mentions among the first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = api.GetStatuses(tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_dict, hashtags_dict = get_mentions_and_hashtags(api, statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = []\n",
    "for (m, n) in mentions_dict.items():\n",
    "    if n > 50:\n",
    "        mentions.append(m)\n",
    "len(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raoult_didier',\n",
       " 'ViolaineGuerin',\n",
       " 'CorinneReverbel',\n",
       " 'momotchiii',\n",
       " 'EChabriere',\n",
       " 'biobiobiobioc',\n",
       " 'ArtLeroux',\n",
       " 'Laissonslespre1',\n",
       " 'medicalfollower',\n",
       " 'Stalec_',\n",
       " 'france_soir',\n",
       " 'DIVIZIO1',\n",
       " 'AssoCovid',\n",
       " 'MartineWonner',\n",
       " 'olivierveran',\n",
       " 'DocteurGonzo4',\n",
       " 'holdup_ledoc',\n",
       " 'IHU_Marseille',\n",
       " 'CNEWS',\n",
       " 'aragon_jb',\n",
       " 'JeanCASTEX',\n",
       " 'silvano_trotta',\n",
       " 'Le___Doc',\n",
       " 'ivanrioufol',\n",
       " 'lemondefr',\n",
       " 'MarianneleMag',\n",
       " 'libe',\n",
       " 'SudRadio',\n",
       " 'VirusWar',\n",
       " 'NicoleDelepine',\n",
       " 'f_philippot',\n",
       " 'OSTERElizabeth1',\n",
       " 'andrebercoff',\n",
       " 'ArianeWalter',\n",
       " 'AnonymeCitoyen',\n",
       " 'franceinfoplus',\n",
       " 'Poulin2012',\n",
       " 'JeanYvesCAPO',\n",
       " 'QuackFighter',\n",
       " 'wargonm',\n",
       " 'noemieschulz',\n",
       " 'LacombeKarine1']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = []\n",
    "for (h, n) in hashtags_dict.items():\n",
    "    if n > 20:\n",
    "        hashtags.append(h)\n",
    "len(hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting ids of mentions and removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_ids = screen_names_to_ids(api, mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_user_ids = np.concatenate((french_user_ids, mentions_ids))\n",
    "french_user_ids = list(set(french_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('user_ids', 'wb') as f:\n",
    "    pickle.dump(french_user_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more general tweets from the HoldUp hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-dfb13dca0246>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'holdup'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'fr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# We look at the oldest and update the values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mlast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "day = 23\n",
    "month = 'Nov'\n",
    "max_id = None\n",
    "tls = []\n",
    "while(day > 9 and month == 'Nov'):\n",
    "    tweets = api.GetSearch(term = 'holdup', count = 100, lang = 'fr', max_id = max_id)\n",
    "    # We look at the oldest and update the values\n",
    "    if len(last) > 0:\n",
    "        last = tweets[-1]\n",
    "        date = last.created_at.split(' ')\n",
    "        day, month = int(date[2]), date[1]\n",
    "        max_id = last.id\n",
    "        # We add the timeline, omitting the last to avoid duplicates\n",
    "        tls.append([x.id for x in tweets[:-1]])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
